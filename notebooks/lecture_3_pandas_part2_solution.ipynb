{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas Part 2 - Advanced Data Manipulation - Solutions\n",
        "\n",
        "This notebook contains solutions to all exercises from Lecture 4: Pandas Part 2.\n",
        "Try solving them yourself first before looking at the solutions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: File I/O - Reading CSV Files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Load and Inspect Sales Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Get the data directory path\n",
        "data_dir = os.path.join('..', 'data')\n",
        "sales_file = os.path.join(data_dir, 'sales_data.csv')\n",
        "\n",
        "# Load the CSV file\n",
        "sales_df = pd.read_csv(sales_file)\n",
        "\n",
        "# Check shape (rows and columns)\n",
        "shape = sales_df.shape\n",
        "print(f\"Shape: {shape[0]} rows, {shape[1]} columns\")\n",
        "\n",
        "# Display first 3 rows\n",
        "first_3 = sales_df.head(3)\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "print(first_3)\n",
        "\n",
        "# Get column names\n",
        "columns_list = sales_df.columns.tolist()\n",
        "print(f\"\\nColumns: {', '.join(columns_list)}\")\n",
        "print(\"\\nGreat! Now we can see our data structure üìä\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: File I/O - Writing Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary = pd.DataFrame({\n",
        "    'Metric': ['Total Customers', 'Average Age', 'Total Sales'],\n",
        "    'Value': [1000, 45.5, 50000]\n",
        "})\n",
        "\n",
        "print(\"Summary DataFrame:\")\n",
        "print(summary)\n",
        "\n",
        "# Save to CSV file\n",
        "summary.to_csv('summary.csv', index=False)\n",
        "print(\"\\n‚úÖ Summary saved to 'summary.csv'!\")\n",
        "print(\"üíæ Always save your processed data for future use!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Data Cleaning - Handling Missing Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Clean Customer Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "\n",
        "# Load customer data\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "# Check for missing Age values\n",
        "missing_ages = customers['Age'].isna().sum()\n",
        "print(f\"Missing ages: {missing_ages}\")\n",
        "\n",
        "# Calculate median age\n",
        "median_age = customers['Age'].median()\n",
        "print(f\"Median age: {median_age:.1f}\")\n",
        "\n",
        "# Fill missing values with median\n",
        "customers_cleaned = customers.copy()\n",
        "customers_cleaned['Age'] = customers_cleaned['Age'].fillna(median_age)\n",
        "\n",
        "# Verify the result\n",
        "remaining_missing = customers_cleaned['Age'].isna().sum()\n",
        "print(f\"\\nRemaining missing ages: {remaining_missing}\")\n",
        "print(\"‚úÖ Data is now clean and ready for analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Data Cleaning - Data Type Conversion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Real-World Case: Fix Data Types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "\n",
        "# Load customer data\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "# Check original data type\n",
        "print(f\"Original SignupDate dtype: {customers['SignupDate'].dtype}\")\n",
        "print(f\"Sample values: {customers['SignupDate'].head()}\")\n",
        "\n",
        "# Convert SignupDate to datetime\n",
        "customers['SignupDate'] = pd.to_datetime(customers['SignupDate'])\n",
        "\n",
        "# Verify the conversion\n",
        "print(f\"\\nAfter conversion dtype: {customers['SignupDate'].dtype}\")\n",
        "print(f\"Sample values: {customers['SignupDate'].head()}\")\n",
        "print(\"\\n‚úÖ SignupDate converted to datetime!\")\n",
        "print(\"Now we can do date operations like filtering by year, month, etc.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Data Cleaning - Removing Duplicates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Clean Duplicate Customers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "\n",
        "# Load customer data\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "print(f\"Original number of customers: {len(customers)}\")\n",
        "\n",
        "# Check for duplicates based on CustomerID\n",
        "dupes = customers.duplicated(subset=['CustomerID']).sum()\n",
        "print(f\"Duplicate CustomerIDs: {dupes}\")\n",
        "\n",
        "# Remove duplicates\n",
        "customers_unique = customers.drop_duplicates(subset=['CustomerID'])\n",
        "\n",
        "print(f\"\\nAfter removing duplicates: {len(customers_unique)} unique customers\")\n",
        "print(\"‚úÖ Dataset is now clean!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Data Cleaning - String Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Extract City Codes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "\n",
        "# Load customer data\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "# Create CityCode column with first 3 letters (uppercase)\n",
        "customers_city_code = customers.copy()\n",
        "customers_city_code['CityCode'] = customers_city_code['City'].str[:3].str.upper()\n",
        "\n",
        "# Display results\n",
        "print(\"Sample of City and CityCode:\")\n",
        "print(customers_city_code[['City', 'CityCode']].head(10))\n",
        "print(\"\\n‚úÖ Extracted first 3 letters as city code!\")\n",
        "print(\"String operations make text processing easy! üìù\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Advanced Operations - Merging DataFrames\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Real-World Case: Combine Customer and Sales Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "sales_file = os.path.join(data_dir, 'sales_data.csv')\n",
        "\n",
        "# Load both datasets\n",
        "customers = pd.read_csv(customer_file)\n",
        "sales = pd.read_csv(sales_file)\n",
        "\n",
        "print(f\"Customers: {len(customers)} rows\")\n",
        "print(f\"Sales: {len(sales)} rows\")\n",
        "\n",
        "# Merge on CustomerID\n",
        "customer_sales = pd.merge(customers, sales, on='CustomerID', how='inner')\n",
        "\n",
        "print(f\"\\nMerged dataset: {len(customer_sales)} rows\")\n",
        "print(\"\\nFirst few rows of merged data:\")\n",
        "print(customer_sales.head())\n",
        "print(\"\\n‚úÖ Now we have customer info with their purchase history! üéØ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Advanced Operations - Pivot Tables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Sales Analysis Pivot Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "sales_file = os.path.join(data_dir, 'sales_data.csv')\n",
        "\n",
        "# Load sales data\n",
        "sales = pd.read_csv(sales_file)\n",
        "\n",
        "# Clean data (remove missing Sales)\n",
        "sales_clean = sales.dropna(subset=['Sales'])\n",
        "\n",
        "# Create pivot table\n",
        "pivot_avg = pd.pivot_table(sales_clean,\n",
        "                          values='Sales',\n",
        "                          index='Product',\n",
        "                          columns='Region',\n",
        "                          aggfunc='mean',\n",
        "                          fill_value=0)\n",
        "\n",
        "print(\"Average Sales by Product and Region:\")\n",
        "print(pivot_avg)\n",
        "print(\"\\n‚úÖ Average sales by product and region!\")\n",
        "print(\"Pivot tables make data analysis visual and intuitive! üìä\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Advanced Operations - Time Series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Real-World Case: Weather Time Series Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "weather_file = os.path.join(data_dir, 'weather_data.csv')\n",
        "\n",
        "# Load weather data\n",
        "weather = pd.read_csv(weather_file)\n",
        "\n",
        "# Convert Date to datetime\n",
        "weather['Date'] = pd.to_datetime(weather['Date'])\n",
        "\n",
        "# Set Date as index\n",
        "weather_indexed = weather.set_index('Date')\n",
        "\n",
        "# Group by month and city to calculate average temperature\n",
        "weather_monthly = weather_indexed.groupby([\n",
        "    weather_indexed.index.to_period('M'), 'City'\n",
        "])['Temperature'].mean().reset_index()\n",
        "\n",
        "# Create pivot table\n",
        "weather_pivot = weather_monthly.pivot(index='Date', columns='City', values='Temperature')\n",
        "\n",
        "print(\"Average Monthly Temperature by City:\")\n",
        "print(weather_pivot.head(10))\n",
        "print(\"\\n‚úÖ Average monthly temperature by city\")\n",
        "print(\"Time series analysis helps identify trends and patterns! üìà\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Advanced Operations - Apply Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úèÔ∏è Challenge: Customer Segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "customer_file = os.path.join(data_dir, 'customer_data.csv')\n",
        "\n",
        "# Load customer data\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "# Define segmentation function\n",
        "def customer_segment(spent):\n",
        "    if spent > 4000:\n",
        "        return 'VIP'\n",
        "    elif spent >= 2000:\n",
        "        return 'Regular'\n",
        "    else:\n",
        "        return 'New'\n",
        "\n",
        "# Apply function to create Segment column\n",
        "customers_segmented = customers.copy()\n",
        "customers_segmented['Segment'] = customers_segmented['TotalSpent'].apply(customer_segment)\n",
        "\n",
        "# Count segments\n",
        "segment_counts = customers_segmented['Segment'].value_counts()\n",
        "\n",
        "print(\"Customer Segments:\")\n",
        "print(segment_counts)\n",
        "print(\"\\nSample of segmented customers:\")\n",
        "print(customers_segmented[['Name', 'TotalSpent', 'Segment']].head(10))\n",
        "print(\"\\n‚úÖ Customer segments created!\")\n",
        "print(\"Apply functions give you unlimited flexibility! üéØ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Project 1: Complete Data Cleaning Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete Data Cleaning Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "sales_file = os.path.join(data_dir, 'sales_data.csv')\n",
        "\n",
        "# Step 1: Load Data\n",
        "raw_data = pd.read_csv(sales_file)\n",
        "print(f\"Step 1: Loaded {len(raw_data)} rows\")\n",
        "\n",
        "# Step 2: Inspect Data\n",
        "missing = raw_data.isna().sum()\n",
        "print(f\"\\nStep 2: Missing values found:\")\n",
        "print(missing)\n",
        "\n",
        "# Step 3: Fix Data Types\n",
        "cleaned_data = raw_data.copy()\n",
        "cleaned_data['Date'] = pd.to_datetime(cleaned_data['Date'])\n",
        "cleaned_data['Sales'] = pd.to_numeric(cleaned_data['Sales'], errors='coerce')\n",
        "cleaned_data['Quantity'] = pd.to_numeric(cleaned_data['Quantity'], errors='coerce')\n",
        "print(\"\\nStep 3: Converted Date, Sales, and Quantity to proper types\")\n",
        "\n",
        "# Step 4: Handle Missing Values\n",
        "sales_median = cleaned_data['Sales'].median()\n",
        "cleaned_data['Sales'] = cleaned_data['Sales'].fillna(sales_median)\n",
        "\n",
        "quantity_median = cleaned_data['Quantity'].median()\n",
        "cleaned_data['Quantity'] = cleaned_data['Quantity'].fillna(quantity_median)\n",
        "\n",
        "print(f\"\\nStep 4: Filled missing Sales with median: {sales_median:.2f}\")\n",
        "print(f\"        Filled missing Quantity with median: {quantity_median:.0f}\")\n",
        "\n",
        "# Step 5: Remove Duplicates\n",
        "before_dedup = len(cleaned_data)\n",
        "cleaned_data = cleaned_data.drop_duplicates()\n",
        "after_dedup = len(cleaned_data)\n",
        "print(f\"\\nStep 5: Removed {before_dedup - after_dedup} duplicate rows\")\n",
        "\n",
        "# Step 6: Final Check\n",
        "final_missing = cleaned_data.isna().sum().sum()\n",
        "print(f\"\\nStep 6: Remaining missing values: {final_missing}\")\n",
        "print(f\"        Final dataset: {len(cleaned_data)} clean rows\")\n",
        "print(\"\\n‚úÖ Data cleaning complete! Dataset is ready for analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Project 2: Comprehensive Sales Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive Sales Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_dir = os.path.join('..', 'data')\n",
        "ecommerce_file = os.path.join(data_dir, 'ecommerce_transactions.csv')\n",
        "customer_file = os.path.join(data_dir, 'customer_demographics.csv')\n",
        "\n",
        "# Step 1: Load and Prepare Data\n",
        "ecommerce = pd.read_csv(ecommerce_file)\n",
        "customers = pd.read_csv(customer_file)\n",
        "\n",
        "# Clean ecommerce data\n",
        "ecommerce['Date'] = pd.to_datetime(ecommerce['Date'])\n",
        "ecommerce['TotalAmount'] = pd.to_numeric(ecommerce['TotalAmount'], errors='coerce')\n",
        "ecommerce_clean = ecommerce.dropna(subset=['TotalAmount'])\n",
        "\n",
        "print(f\"Step 1: Loaded {len(ecommerce_clean)} transactions\")\n",
        "\n",
        "# Step 2: Revenue by Product Category\n",
        "revenue_by_category = ecommerce_clean.groupby('Category')['TotalAmount'].sum().sort_values(ascending=False)\n",
        "print(\"\\nStep 2: Revenue by Category:\")\n",
        "print(revenue_by_category)\n",
        "\n",
        "# Step 3: Best Performing Region\n",
        "revenue_by_region = ecommerce_clean.groupby('Region')['TotalAmount'].sum().sort_values(ascending=False)\n",
        "best_region = revenue_by_region.index[0]\n",
        "print(f\"\\nStep 3: üèÜ Best performing region: {best_region}\")\n",
        "print(\"Revenue by Region:\")\n",
        "print(revenue_by_region)\n",
        "\n",
        "# Step 4: Monthly Sales Trends\n",
        "ecommerce_clean['Month'] = ecommerce_clean['Date'].dt.to_period('M')\n",
        "monthly_revenue = ecommerce_clean.groupby('Month')['TotalAmount'].sum()\n",
        "print(\"\\nStep 4: Monthly Revenue Trends:\")\n",
        "print(monthly_revenue.head(10))\n",
        "\n",
        "# Step 5: Top Customers by Spending\n",
        "customer_sales = ecommerce_clean.groupby('CustomerID')['TotalAmount'].sum().sort_values(ascending=False).head(10)\n",
        "print(\"\\nStep 5: Top 10 Customers by Spending:\")\n",
        "print(customer_sales)\n",
        "\n",
        "# Step 6: Payment Method Analysis\n",
        "payment_analysis = ecommerce_clean.groupby('PaymentMethod').agg({\n",
        "    'TotalAmount': ['sum', 'mean', 'count']\n",
        "})\n",
        "print(\"\\nStep 6: Payment Method Analysis:\")\n",
        "print(payment_analysis)\n",
        "\n",
        "# Step 7: Merge with Customer Demographics\n",
        "top_customers_df = pd.DataFrame({\n",
        "    'CustomerID': customer_sales.index,\n",
        "    'TotalSpent': customer_sales.values\n",
        "})\n",
        "\n",
        "top_customers_info = pd.merge(top_customers_df, customers, on='CustomerID', how='left')\n",
        "print(\"\\nStep 7: Top Customers with Demographics:\")\n",
        "print(top_customers_info[['CustomerID', 'TotalSpent', 'FirstName', 'City', 'Age']].head())\n",
        "\n",
        "# Step 8: Summary Statistics\n",
        "total_revenue = ecommerce_clean['TotalAmount'].sum()\n",
        "avg_transaction = ecommerce_clean['TotalAmount'].mean()\n",
        "total_transactions = len(ecommerce_clean)\n",
        "avg_discount = ecommerce_clean['Discount'].mean()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä EXECUTIVE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
        "print(f\"Average Transaction: ${avg_transaction:.2f}\")\n",
        "print(f\"Total Transactions: {total_transactions}\")\n",
        "print(f\"Average Discount: {avg_discount:.1f}%\")\n",
        "print(f\"Best Category: {revenue_by_category.index[0]} (${revenue_by_category.iloc[0]:,.2f})\")\n",
        "print(f\"Best Region: {best_region} (${revenue_by_region[best_region]:,.2f})\")\n",
        "print(\"\\n‚úÖ Comprehensive analysis complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
